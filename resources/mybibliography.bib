@article{bastani2018verifiable,
  title={Verifiable reinforcement learning via policy extraction},
  author={Bastani, Osbert and Pu, Yewen and Solar-Lezama, Armando},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{costa_recent_2023,
	title = {Recent advances in decision trees: an updated survey},
	volume = {56},
	issn = {1573-7462},
	url = {https://doi.org/10.1007/s10462-022-10275-5},
	doi = {10.1007/s10462-022-10275-5},
	abstract = {Decision Trees ({DTs}) are predictive models in supervised learning, known not only for their unquestionable utility in a wide range of applications but also for their interpretability and robustness. Research on the subject is still going strong after almost 60 years since its original inception, and in the last decade, several researchers have tackled key matters in the field. Although many great surveys have been published in the past, there is a gap since none covers the last decade of the field as a whole. This paper proposes a review of the main recent advances in {DT} research, focusing on three major goals of a predictive learner: issues regarding the fitting of training data, generalization, and interpretability. Moreover, by organizing several topics that have been previously analyzed in isolation, this survey attempts to provide an overview of the field, its key concerns, and future trends, serving as a good entry point for both researchers and newcomers to the machine learning community.},
	pages = {4765--4800},
	number = {5},
	journaltitle = {Artificial Intelligence Review},
	shortjournal = {Artificial Intelligence Review},
	author = {Costa, Vinícius G. and Pedreira, Carlos E.},
	date = {2023-05-01},
}

@inproceedings{mazumder_quant-bnb_2022,
	title = {Quant-{BnB}: A Scalable Branch-and-Bound Method for Optimal Decision Trees with Continuous Features},
	volume = {162},
	url = {https://proceedings.mlr.press/v162/mazumder22a.html},
	series = {Proceedings of Machine Learning Research},
	abstract = {Decision trees are one of the most useful and popular methods in the machine learning toolbox. In this paper, we consider the problem of learning optimal decision trees, a combinatorial optimization problem that is challenging to solve at scale. A common approach in the literature is to use greedy heuristics, which may not be optimal. Recently there has been significant interest in learning optimal decision trees using various approaches (e.g., based on integer programming, dynamic programming)—to achieve computational scalability, most of these approaches focus on classification tasks with binary features. In this paper, we present a new discrete optimization method based on branch-and-bound ({BnB}) to obtain optimal decision trees. Different from existing customized approaches, we consider both regression and classification tasks with continuous features. The basic idea underlying our approach is to split the search space based on the quantiles of the feature distribution—leading to upper and lower bounds for the underlying optimization problem along the {BnB} iterations. Our proposed algorithm Quant-{BnB} shows significant speedups compared to existing approaches for shallow optimal trees on various real datasets.},
	pages = {15255--15277},
	booktitle = {Proceedings of the 39th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Mazumder, Rahul and Meng, Xiang and Wang, Haoyue},
	editor = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	date = {2022-07-17},
}